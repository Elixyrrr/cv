[{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\nDon’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution. ","permalink":"http://localhost:1313/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"RAG (Retrieval-Augmented Generation) « est une méthode qui enrichit les prompts du LLM avec un contexte pertinent récupéré pour réduire les hallucinations et améliorer la précision factuelle. » .Dans ce projet on utilise Ollama pour exécuter un LLM local qui, à partir de triplets récupérés par FAISS depuis un graphe de connaissance, va évaluer la qualité du graphe en comparant les réponses générées aux données sources.\n","permalink":"http://localhost:1313/projects/creations/kg/","tags":["LLM","RAG","Knowledge Graph","LangChain","FAISS","MongoDB","Deep Learning","NLP"],"title":"Évaluation de la qualité d’un graphe de connaissance avec un pipeline RAG"},{"categories":null,"contents":"Les bases de données traditionnelles Oracle, SQL Server DB2 ont été conçues pour garantir l\u0026rsquo;accès concurrent aux données, l\u0026rsquo;aspect transactionnel des traitements, pour pouvoir accepter des centaines de sessions utilisateurs, pour fournir des métriques précises sur l\u0026rsquo;utilisation du système au niveau de processeur, mémoire, réseau.\nCes bases de données sont donc plus lentes en terme de consultation de données massives et non structurées.\nDes nouvelles technologies comme Hadoop sont orientées plus vers la consultation des données, traitement massif des données, structurées ou non structurés.\nHadoop implémente la technologie du sharding, qui distribue le travail sur plusieurs nœuds et permet de cette façon de traiter une masse de données importante. Les outils qui accompagnent Hadoop sont Hive, Hbase, Spark.\nCe projet porte sur le développement d\u0026rsquo;un nouveau logiciel orientée Big Data, appelé Bayesian Database, qui va collectionner les données liées aux différentes applications d’une entreprise et les traiter via les méthodes Machine Learnng en utilisant Hadoop.\n","permalink":"http://localhost:1313/projects/creations/bayesian/","tags":["Big Data","Hadoop","Base Bayésienne","Machine Learning","Spark","Hive"],"title":"Application Big Data"},{"categories":null,"contents":"Ce projet consistait à développer un outil capable d’analyser des contenus publiés sur les réseaux sociaux pour détecter des messages témoignant de signes dépressifs, en utilisant des techniques de deep learning tels que \u0026ldquo;FCL\u0026rdquo; ou bien de machine learning tels que \u0026ldquo;SVM\u0026rdquo; et \u0026ldquo;Random Forest\u0026rdquo;\n","permalink":"http://localhost:1313/projects/creations/depression/","tags":["NLP","Machine Learning","Deep Learning","Big Data"],"title":"Détection de signes dépressifs dans les publications sur les réseaux sociaux"},{"categories":null,"contents":"Le projet consistait à créer une application web de jeu d’échecs conforme à une architecture REST (Répresentationnal State Transfer) et au pattern MVC (Modèle-Vue-Contrôleur). Les deux joueurs peuvent s’affronter en temps réel via Socket.IO, et chaque partie est sauvegardée dans MongoDB (ou SQL) pour être reprise coup par coup. Des fonctionnalités annexes, comme le chronomètre, la demande de partie à l’adversaire et la possibilité de rejouer un coup, ont été implémentées. En bonus, une IA basique a été développée pour permettre à l’utilisateur de jouer contre l’ordinateur. Les technologies employées incluent Node.js, Express, Socket.IO et MongoDB. Le site prend un peu de temps à se lancer.\n","permalink":"http://localhost:1313/projects/creations/chess/","tags":["Node.js","Express","Socket.IO","MongoDB","REST","Machine Learning"],"title":"Jeu d’échecs en ligne"},{"categories":null,"contents":"Ce projet consiste à combler les valeurs manquantes dans des séries temporelles de consommation électrique résidentielle, mesurées toutes les 30 minutes. Nous disposons de 3 127 séries d’entraînement de 12 864 relevés chacune, sans valeurs manquantes, puis de jeux de test comportant environ 30 % de mesures masquées aléatoirement. Pour restaurer ces données, nous avons implémenté et comparé plusieurs approches : méthodes statistiques classiques (interpolations, moyennes mobiles), algorithmes de machine learning (régressions, forêts aléatoires) et modèles de deep learning (LSTM, CNN, autoencodeurs). Chaque méthode est évaluée à l’aide de l’erreur moyenne absolue (MAE) entre les valeurs prédictes et réelles, afin de déterminer la solution la plus efficace pour reconstituer fidèlement les courbes de consommation.\n","permalink":"http://localhost:1313/projects/creations/imputation/","tags":["Time Series","Machine Learning","Deep Learning"],"title":"Imputation des valeurs manquantes dans les séries temporelles d’électricité"},{"categories":null,"contents":"Intro Doesn\u0026rsquo;t matter whether it\u0026rsquo;s a CakePHP app for a client, your own personal CMS, or any other web based application. If your passing around passwords or other sensitive info you should really implement SSL. SSL provides 2 main perks to your visitors.\nFirst it encrypts all communication that flies across the web. This prevents curious or devious billies from getting your secrets. Secondly it ensures to the user that your server is in fact who it claims, and not a nasty \u0026lsquo;man in the middle\u0026quot; attack. Finally it gives your site that touch of class\u0026hellip;. which of course a classy person like yourself relies on. Once you implement SSL certificates on your server you\u0026rsquo;ll want to require secure connections using Apache\u0026rsquo;s rewrite module. Now I won\u0026rsquo;t dwell on the creation and signing of certificates, its already well documented. If your just starting out though,heres a few links I recommend;\nCreating self-signed certificates (free, but should only be used internally or for testing, users will; see an \u0026lsquo;Untrusted\u0026quot; warning) Requesting a CA Signed certificate (not free, but the final certificate is trusted and seamless for users) The second link uses the schools internal CA, you will need to pay a public CA like Entrust or Verisign. All of this information is aimed at \u0026rsquo;nix or solaris servers running apache. Why? cause a production windows server is laughable :-p\nNow that you have a certificate, whats next? So there you are you have a shiny new Certificate and Server key, how do you force visitors to your apache driven site to use the SSL? You copied the certificates into the appropite locations right? And you have made the needed changes in httpd.conf right? So now when you view https://example.com you see a \u0026rsquo;trusted\u0026rsquo; warning or your site right? If No to any of these than this article does a pretty good job of outlining those steps.\nThe SSL Works, How do I force connections to use it? First you need to decide if you want to force every page on your site to use SSL, or only a particular sub-domain, or maybe just your admin directory. Since the overhead is minimal there is no harm is forcing the entire domain to leverage SSL, but if it is a self-signed certificate for your personal use than you\u0026rsquo;ll most certainly want to restrict its use to your own areas. This prevents users from seeing that nasty warning \u0026ldquo;This server is not trusted\u0026rdquo; You\u0026rsquo;ll know if your using SSL because the url prefix changes from http to https (s for secure).\nForcing entire domain to use SSL You want any visit, any where to use ssl. This probably the simplest solution. Create or append to your htaccess file in the top directory of your server. Some people use a port check (80 is typically http, while 443 is https) but if you have alernate configs or the user just adds :8080 to the end of the url this method is useless. Instead check whether the https environmental variable is set, if not then redirect.\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://%{SERVER_NAME}$1 \\[R,L\\] Forcing sub-domains to use SSL Maybe you only want mysecretarea.example.com to use SSL, that\u0026rsquo;s easy enough. Its the same premise as above, but you move the htaccess file into the directory that corresponds to the subdomain. Also change the second line like below;\nRewriteCond %{HTTPS} !=on RewriteRule ^(.*)$ https://mysecretarea.%{SERVER_NAME}$1 \\[R,L\\] Forcing a directory to use SSL This method cn get a little hairier if your using aliases or redirects on top of this one. You\u0026rsquo;ll need to consider what order the commands are read. The basic principle is like so. You want all visits to example.com/admin to use ssl. Create a htaccess file in the parent directory. Again will check for the https variable, but this time we also check for the sub-directory to be in the path.\nRewriteCond %{HTTPS} !=on RewriteRule ^/admin/(.*)$ https://%{SERVER_NAME}/admin/$1 \\[R,L\\] ","permalink":"http://localhost:1313/blog/force-ssl/","tags":["apache","apache","redirect","rewrite","ssl","web development"],"title":"Forcing Visits to use SSL"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"http://localhost:1313/search/","tags":null,"title":"Search Results"}]